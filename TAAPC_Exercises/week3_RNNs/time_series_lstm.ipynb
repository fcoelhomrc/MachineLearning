{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ba88fd",
   "metadata": {},
   "source": [
    "# Time series forecasting\n",
    "\n",
    "In this notebook, we will use LSTMs for weather forecasting.\n",
    "\n",
    "The notebook is based on this TensorFlow tutorial: https://www.tensorflow.org/tutorials/structured_data/time_series. However, here we will use [PyTorch](https://pytorch.org/) to implement our model.\n",
    "\n",
    "The dataset we use here was recorded by the [Max Planck Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/). This dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. These were collected every 10 minutes, beginning in 2003. For efficiency, you will use only the data collected between 2009 and 2016. This section of the dataset was prepared by FranÃ§ois Chollet for his book [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "import requests\n",
    "import zipfile\n",
    "response = requests.get('https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip')\n",
    "open('weather_dataset.zip', 'wb').write(response.content)\n",
    "with zipfile.ZipFile('weather_dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')\n",
    "df = pd.read_csv('jena_climate_2009_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the data frame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090ee59",
   "metadata": {},
   "source": [
    "The dataset contains measurements taken each 10 minutes. We will subsample it to have hourly measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6088a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice [start:stop:step], starting from index 5 take every 6th record.\n",
    "df = df[5::6]\n",
    "\n",
    "date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca04d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = ['T (degC)', 'p (mbar)', 'rho (g/m**3)']\n",
    "plot_features = df[plot_cols]\n",
    "plot_features.index = date_time\n",
    "_ = plot_features.plot(subplots=True)\n",
    "\n",
    "plot_features = df[plot_cols][:480]\n",
    "plot_features.index = date_time[:480]\n",
    "_ = plot_features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d7592",
   "metadata": {},
   "source": [
    "Let's look at some statistics of our data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be0a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid values\n",
    "wv = df['wv (m/s)']\n",
    "bad_wv = (wv == -9999.0)\n",
    "wv[bad_wv] = 0.0\n",
    "\n",
    "max_wv = df['max. wv (m/s)']\n",
    "bad_max_wv = (max_wv == -9999.0)\n",
    "max_wv[bad_max_wv] = 0.0\n",
    "\n",
    "# The above inplace edits are reflected in the DataFrame.\n",
    "df['wv (m/s)'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d8d1f3",
   "metadata": {},
   "source": [
    "Our dataset contains angular data for the wind direction. However, angles are not suitable representations as model inputs (not that 0 and 360 degrees are the same). Moreover, whenever the wind speed is zero, the wind direction is meaningless.\n",
    "\n",
    "Therefore, we will use the wind speed and direction to build the corresponding **wind vector**, which will be one of the inputs of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa308196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build wind vector\n",
    "wv = df.pop('wv (m/s)')\n",
    "max_wv = df.pop('max. wv (m/s)')\n",
    "\n",
    "# Convert to radians.\n",
    "wd_rad = df.pop('wd (deg)')*np.pi / 180\n",
    "\n",
    "# Calculate the wind x and y components.\n",
    "df['Wx'] = wv*np.cos(wd_rad)\n",
    "df['Wy'] = wv*np.sin(wd_rad)\n",
    "\n",
    "# Calculate the max wind x and y components.\n",
    "df['max Wx'] = max_wv*np.cos(wd_rad)\n",
    "df['max Wy'] = max_wv*np.sin(wd_rad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc23cb",
   "metadata": {},
   "source": [
    "Date strings are also innapropriate as model inputs. Can you understand the preprocessing we are doing below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "\n",
    "plt.plot(np.array(df['Day sin'])[:25])\n",
    "plt.plot(np.array(df['Day cos'])[:25])\n",
    "plt.xlabel('Time [h]')\n",
    "plt.title('Time of day signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4d599b",
   "metadata": {},
   "source": [
    "**Explain** the preprocessing above in your own words:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bf2c7d",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c058b2",
   "metadata": {},
   "source": [
    "Now, we are ready to split our data into train, validation and test splits. We will use the first 70% samples for training, the following 20% for validation and the last 10% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bcc2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "\n",
    "n = len(df)\n",
    "train_df = df[0:int(n*0.7)]\n",
    "val_df = df[int(n*0.7):int(n*0.9)]\n",
    "test_df = df[int(n*0.9):]\n",
    "\n",
    "num_features = df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0d811",
   "metadata": {},
   "source": [
    "Note that we are **not** splitting our data randomly. Can you explain why a random split is not a good idea in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e300b8f3",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c8b20",
   "metadata": {},
   "source": [
    "It is crucial to scale features before feeding them through a neural network. Normalize the data in the cell below by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "You should create the variables `train_mean` and `train_std` and use them to normalize the three data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da84001",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "\n",
    "## *** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at normalized data\n",
    "df_std = (df - train_mean) / train_std\n",
    "df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
    "_ = ax.set_xticklabels(df.keys(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02167924",
   "metadata": {},
   "source": [
    "The `torch.utils.data.Dataset` class provides a very convenient way of storing and loading data. In the cell below, we define the class `WeatherData`, which inherits from this `Dataset` class. The following functions must be defined within the class:\n",
    "- `__init__`, where the attributes of the class are defined;\n",
    "- `__getitem__`, which gets an index (`int`) and returns the data example corresponding to the provided index (can be a `tuple` or a `dict`);\n",
    "- `__len__`, which returns the length of the dataset.\n",
    "\n",
    "We have already written the functions `__getitem__` and `__len__` for you. All you need to do is write the code for the `__init__` function. There, you should create two lists as attributes of the class: `inputs` and `targets`. The `i`-th element of the list `inputs` should be a `torch.FloatTensor` containing a slice of the data with `input_width` consecutive samples. The `i`-th element of the list `targets` should be a `torch.FloatTensor` containing a slice of the data with the `target_width` consecutive samples that follow after `inputs[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherData(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, input_width, target_width):\n",
    "        ## YOUR CODE HERE ##\n",
    "        \n",
    "        ## *** ##               \n",
    "    def __getitem__(self, idx):\n",
    "        return {'inputs': self.inputs[idx],\n",
    "                'targets': self.targets[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(len(self.inputs), len(self.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ceb51d",
   "metadata": {},
   "source": [
    "We will use the observations of the previous 24 hours to predict the temperature for the next 24 hours. Thus, we will set both `input_width` and `target_width` to 24 in the instances of the dataset. After you complete this exercise, you can try with different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cfd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_WIDTH = 24\n",
    "TARGET_WIDTH = 24\n",
    "\n",
    "train_data = WeatherData(train_df, INPUT_WIDTH, TARGET_WIDTH)\n",
    "val_data = WeatherData(val_df, INPUT_WIDTH, TARGET_WIDTH)\n",
    "test_data = WeatherData(test_df, INPUT_WIDTH, TARGET_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize some examples\n",
    "idx = random.randint(0, len(train_data))\n",
    "inputs = train_data[idx]['inputs'].numpy()\n",
    "targets = train_data[idx]['targets'].numpy()\n",
    "\n",
    "for i in range(len(train_df.columns)):\n",
    "    plt.plot(range(INPUT_WIDTH), inputs[:,i], label='input')\n",
    "    plt.plot(range(INPUT_WIDTH, INPUT_WIDTH + TARGET_WIDTH), targets[:,i], label='target')\n",
    "    plt.ylabel(train_df.columns[i])\n",
    "    plt.xlabel('Time')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c307366",
   "metadata": {},
   "source": [
    "Now, it's time to define our neural network! It will consist of a `torch.nn.LSTMCell` with a state dimension defined by `hidden_size`, followed by a `torch.nn.Linear` layer that projects from the hidden space to the output.\n",
    "\n",
    "The best way to define a neural network in PyTorch is by extending the class `torch.nn.Module`. For this purpose, you need to define two functions:\n",
    "- `__init__`, where all the layers of the network are instantiated;\n",
    "- `forward`, where the forward pass is defined.\n",
    "\n",
    "Complete the two methods in the cell below. The comments throughout the code will provide you some hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4089783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherPredictor(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # instantiate a torch.nn.LSTMCell and a torch.nn.Linear layer\n",
    "        ## YOUR CODE HERE ##\n",
    "        \n",
    "        ## *** ##\n",
    "        \n",
    "    def forward(self, inputs, num_steps):\n",
    "        inputs = inputs.transpose(0, 1)  # batch, time, dim -> time, batch, dim\n",
    "        \n",
    "        # we initialize the internal states of the LSTM with zeros\n",
    "        hx = torch.zeros((inputs.shape[1], self.hidden_size), device=inputs.device)\n",
    "        cx = torch.zeros((inputs.shape[1], self.hidden_size), device=inputs.device)\n",
    "        \n",
    "        # warmup: feed the inputs through the LSTM one by one from t = 0 up to t = T-1\n",
    "        ## YOUR CODE HERE ##\n",
    "        \n",
    "        ## *** ##\n",
    "        \n",
    "        # predict num_steps into the future autoregressively\n",
    "        outputs = []\n",
    "        input_t = inputs[-1]\n",
    "        ## YOUR CODE HERE ##\n",
    "        \n",
    "        ## *** ##\n",
    "        \n",
    "        # stack the list of outputs into a single tensor\n",
    "        outputs = torch.stack(outputs)\n",
    "        \n",
    "        outputs = outputs.transpose(0, 1)  # time, batch, dim -> batch, time, dim\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe55dc",
   "metadata": {},
   "source": [
    "Now we are going to define the evaluation and training loops of our model in the functions `evaluate` and `fit`, respectively. This should be identical to what you may have done before for non-recurrent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d3383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, **kwargs):\n",
    "    loss_fn = kwargs.get('loss_fn', torch.nn.functional.mse_loss)\n",
    "    device = kwargs.get('device', torch.device('cpu'))\n",
    "    \n",
    "    model.eval()\n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    avg_loss = 0.\n",
    "    for i, batch in pbar:\n",
    "        ## YOUR CODE HERE ##\n",
    "        \n",
    "        ## *** ##\n",
    "        pbar.set_description(f'loss = {loss:.3f}')\n",
    "    avg_loss /= len(val_loader)\n",
    "    return avg_loss\n",
    "\n",
    "def fit(model, train_loader, val_loader, optimizer, **kwargs):\n",
    "    num_epochs = kwargs.get('num_epochs', 100)\n",
    "    loss_fn = kwargs.get('loss_fn', torch.nn.functional.mse_loss)\n",
    "    device = kwargs.get('device', torch.device('cpu'))\n",
    "    \n",
    "    train_loss_hist, val_loss_hist = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        \n",
    "        print('Training phase...')\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        for i, batch in pbar:\n",
    "            ## YOUR CODE HERE ##\n",
    "            \n",
    "            ## *** ##\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_description(f'loss = {loss:.3f}')\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f'train loss = {train_loss:.3f}')\n",
    "        train_loss_hist.append(train_loss)\n",
    "        \n",
    "        print('Validation phase...')\n",
    "        val_loss = evaluate(model, val_loader, loss_fn=loss_fn, device=device)\n",
    "        print(f'validation loss = {val_loss:.3f}')\n",
    "        val_loss_hist.append(val_loss)\n",
    "        \n",
    "    return train_loss_hist, val_loss_hist\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27f8b5",
   "metadata": {},
   "source": [
    "The only thing that is still missing are the `torch.utils.data.DataLoader`s. These provide a very convenient way of aggregating the data into mini-batches. We have already defined them for you in the cell below.\n",
    "\n",
    "We have also defined a set of hyperparameters (`HIDDEN_SIZE`, `LEARNING_RATE`, `NUM_EPOCHS`, and `BATCH_SIZE`). You should understand the role of each of these and you can experiment with different values later on.\n",
    "\n",
    "The code is also prepared to run using CUDA if you have a CUDA-capable GPU in your computer. The commented code is for running on Mac M1 GPUs. However, as of today, the training will be faster on Mac M1 CPU than on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print('DEVICE:', DEVICE)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = WeatherPredictor(train_data[0]['inputs'].shape[1], HIDDEN_SIZE).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08345c83",
   "metadata": {},
   "source": [
    "Train the model by running the cell below. It may take a while depending on your hardware and on the `NUM_EPOCHS` that you have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = fit(model, train_loader, val_loader, optimizer, num_epochs=NUM_EPOCHS, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Train and validation losses')\n",
    "plt.plot(range(len(train_loss)), train_loss, label='train loss')\n",
    "plt.plot(range(len(val_loss)), val_loss, label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63981388",
   "metadata": {},
   "source": [
    "Save the trained model to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d315864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d010556",
   "metadata": {},
   "source": [
    "Compute the loss on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(model, val_loader, device=DEVICE)\n",
    "print(f'Test loss = {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1034faff",
   "metadata": {},
   "source": [
    "Now you will implement a `predict` function, which returns a `list` with the predictions of your model for each of the examples in the `data_loader`. This should be very similar to the `evaluate` function that you've written before, except that now there is no loss to be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f0f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, **kwargs):\n",
    "    num_steps = kwargs.get('num_steps', 24)\n",
    "    device = kwargs.get('device', torch.device('cpu'))\n",
    "    \n",
    "    model.eval()\n",
    "    preds = []\n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for i, batch in pbar:\n",
    "        ## YOUR CODE HERE ##\n",
    "        \n",
    "        ## *** ##\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = predict(model, test_loader, num_steps=TARGET_WIDTH, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a6fe8",
   "metadata": {},
   "source": [
    "Finally, let's visualize a few predictions of our model. You can run the cell below multiple times to visualize the temperature predictions for different examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, len(test_data))\n",
    "inputs = test_data[idx]['inputs'].numpy()\n",
    "targets = test_data[idx]['targets'].numpy()\n",
    "preds = test_preds[idx].numpy()\n",
    "\n",
    "for i in [1, 2, 3]:\n",
    "    plt.plot(range(INPUT_WIDTH), inputs[:,i], label='input')\n",
    "    plt.plot(range(INPUT_WIDTH, INPUT_WIDTH + TARGET_WIDTH), targets[:,i], label='target')\n",
    "    plt.plot(range(INPUT_WIDTH, INPUT_WIDTH + TARGET_WIDTH), preds[:,i], label='prediction')\n",
    "    plt.ylabel(test_df.columns[i])\n",
    "    plt.xlabel('Time')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e6355b",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "Now, it's time for you to be creative! Try to find ways of improving the model. You can start by playing around with hyperparameters and then going for more complex stuff, like adding more layers or even trying with a different architecture. Perhaps, on this problem, a convolutional network works better than a recurrent one, who knows?\n",
    "\n",
    "**Don't be afraid to try! Good luck! :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbbd937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Keras')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3682e2fce7b21f883e3d300ad20e297d7abf3ac133eb069e4f98235872e7ffc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
