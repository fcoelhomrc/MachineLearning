{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "11cff105",
      "metadata": {
        "id": "11cff105"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b65d87",
      "metadata": {
        "id": "06b65d87"
      },
      "source": [
        "In this notebook, you'll implement the **skip-gram** version of the Word2Vec model that was presented in the lecture. Please refer to the slides and to the original paper (https://arxiv.org/abs/1301.3781) whenever needed.\n",
        "\n",
        "We'll use the WikiText2 dataset, which is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. \n",
        "\n",
        "To split the text into words, we shall use the 'basic_english' tokenizer from torchtext (https://pytorch.org/text/stable/index.html). This is a word-level tokenizer that splits a text into a list of English uncased words and punctuation symbols. Note that a special '\\<unk>' token is used whenever an unknown word is found."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA2iQfRkswHx",
        "outputId": "f8fb6d62-8a66-49da-cdcf-ab914641a0ea"
      },
      "id": "lA2iQfRkswHx",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.25.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.5.1)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchdata) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b82c5d03",
      "metadata": {
        "id": "b82c5d03"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data import get_tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e5595b",
      "metadata": {
        "id": "24e5595b"
      },
      "source": [
        "You should start by building `vocab`, a dictionary containing the vocabulary. The keys will be the words and the values are the corresponding indices, starting from 0. It is a good practice to discard rare words from the vocabulary, so we'll only keep those words that appear at least `MIN_REPS` times.\n",
        "\n",
        "Thus, you should do the following in the cell below:\n",
        "\n",
        "1. Build a `word_count` dictionary containing the number of times that each word appears. For this purpose, you should create an iterator from `train_iter` and iterate on it to get each sequence in the dataset. Each sequence should be split in a list of tokens (words) by calling the `tokenizer`. \n",
        "\n",
        "2. Build a `vocab` dictionary as described above. The `vocab` should only contain valid words that appear at least `MIN_REPS`. We have reserved a special index (-100) for the '\\<unk>' token which you should use for out-of-vocabulary words.\n",
        "    \n",
        "It is also useful to have the inverse mapping from `vocab`, i.e. another dictionary that, given the word index, outputs the corresponding word. We have already built that for you in `vocab_inv`.\n",
        "\n",
        "**Remark:** We could be using `torchtext.vocab`, which facilitates building the `vocab` dictionary and its inverse. For pedagogical reasons, this time you will implement these by yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "31bc6a31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31bc6a31",
        "outputId": "0c43a186-9d6a-4fc7-b257-5e9aab7fb112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary has 4098 different words.\n"
          ]
        }
      ],
      "source": [
        "MIN_REPS = 50 \n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "word_count = {}\n",
        "for sequence in train_iter:\n",
        "    tokens = tokenizer(sequence)\n",
        "    for idx, tk in enumerate(tokens):\n",
        "        if tk not in word_count:\n",
        "            word_count[tk] = 1\n",
        "        else:\n",
        "            word_count[tk] += 1\n",
        "vocab = {}\n",
        "idx = 0\n",
        "for tk, count in word_count.items():\n",
        "    if count >= MIN_REPS and tk != \"<unk>\":\n",
        "        vocab[tk] = idx\n",
        "        idx += 1\n",
        "\n",
        "### *** ###\n",
        "vocab['<unk>'] = -100\n",
        "vocab_inv = {val: key for (key, val) in vocab.items()}\n",
        "\n",
        "print(f'Vocabulary has {len(vocab)-1} different words.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(vocab_inv.get(i, \"KEY_ERROR\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRjPCSH8vdv4",
        "outputId": "a90d4645-95c5-482b-ca37-33e4108167cb"
      },
      "id": "vRjPCSH8vdv4",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=\n",
            "valkyria\n",
            "chronicles\n",
            "iii\n",
            "no\n",
            "3\n",
            "(\n",
            "japanese\n",
            ",\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a959066",
      "metadata": {
        "id": "0a959066"
      },
      "source": [
        "Now it is time to complete the `SkipGramDataset` class by implementing its `__init__` method. This method receives `train_iter`, `vocab`, and the width of the window for the skip gram data. For each word, you will consider all the words that appear up to `window_width` words **before or after** the word, so the **total sequence length will be `2*window_width + 1`**.\n",
        "\n",
        "In this `__init__` method you should build (input, context) pairs for the skip gram model and store them in the list attributes `self.inputs` and `self.contexts`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "852cf82c",
      "metadata": {
        "id": "852cf82c"
      },
      "outputs": [],
      "source": [
        "class SkipGramDataset(Dataset):\n",
        "    def __init__(self, train_iter, vocab, window_width):\n",
        "        ### YOUR CODE HERE ###\n",
        "        self.inputs = []\n",
        "        self.contexts = []\n",
        "        for sequence in train_iter:\n",
        "            tokens = tokenizer(sequence)\n",
        "            length = len(tokens)\n",
        "            for i, tk in enumerate(tokens):\n",
        "                if i < window_width or i >= length - window_width:\n",
        "                    continue\n",
        "                if tk not in vocab or tk == \"<unk>\":\n",
        "                    continue\n",
        "                for j in range(i - window_width, i + window_width + 1):\n",
        "                    if j == i:\n",
        "                        continue\n",
        "                    next = tokens[j]\n",
        "                    if next not in vocab or next == \"<unk>\":\n",
        "                        continue\n",
        "                    else:\n",
        "                        self.inputs.append(vocab[tk])\n",
        "                        self.contexts.append(vocab[next])\n",
        "        ### *** ###\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.inputs[index], self.contexts[index]\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "92d2b125",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92d2b125",
        "outputId": "3d302f70-9a6c-4a6c-c55f-361d5f22ab05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/iter/combining.py:249: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset has 10647430 examples.\n"
          ]
        }
      ],
      "source": [
        "WINDOW_WIDTH = 4\n",
        "\n",
        "dataset = SkipGramDataset(train_iter, vocab, WINDOW_WIDTH)\n",
        "print(f'Dataset has {len(dataset)} examples.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for input in dataset.inputs:\n",
        "    if input < 0:\n",
        "        count += 1\n",
        "\n",
        "for context in dataset.contexts:\n",
        "    if context < 0:\n",
        "        count += 1\n",
        "\n",
        "count"
      ],
      "metadata": {
        "id": "zsnCbUrj35vD",
        "outputId": "2585009f-875c-468c-84d9-4fa15b3112a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zsnCbUrj35vD",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abecafcc",
      "metadata": {
        "id": "abecafcc"
      },
      "source": [
        "Let's look at some word pairs..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "12b25756",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12b25756",
        "outputId": "5a9fc4ae-7a45-4a62-af2e-294e0bf7106b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "by developed\n",
            "by and\n",
            "by media\n",
            "by .\n",
            "and game\n",
            "and developed\n",
            "and by\n",
            "and media\n",
            "and .\n",
            "and vision\n",
            "and for\n",
            "media developed\n",
            "media by\n",
            "media and\n",
            "media .\n",
            "media vision\n",
            "media for\n",
            "media the\n",
            ". by\n",
            ". and\n",
            ". media\n",
            ". vision\n",
            ". for\n",
            ". the\n",
            ". playstation\n",
            "vision and\n",
            "vision media\n",
            "vision .\n",
            "vision for\n",
            "vision the\n",
            "vision playstation\n",
            "for and\n",
            "for media\n",
            "for .\n",
            "for vision\n",
            "for the\n",
            "for playstation\n",
            "for .\n",
            "the media\n",
            "the .\n",
            "the vision\n",
            "the for\n",
            "the playstation\n",
            "the .\n",
            "the released\n",
            "playstation .\n",
            "playstation vision\n",
            "playstation for\n",
            "playstation the\n",
            "playstation .\n"
          ]
        }
      ],
      "source": [
        "for i in range(200, 250):\n",
        "    x, y = dataset[i]\n",
        "    x_word = vocab_inv[x]\n",
        "    y_word = vocab_inv[y]\n",
        "    print(x_word, y_word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8af9b9a",
      "metadata": {
        "id": "b8af9b9a"
      },
      "source": [
        "Let's implement the word2vec model. As you can see from the lecture slides, this model simply consists of an `nn.Embedding` layer mapping word indices to embeddings and a `nn.Linear` projection layer mapping to the vocabulary dimension. As you may already know, you should **not** apply the softmax at the output. The loss function you will use does that for you and in a more numerically stable way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "11f11333",
      "metadata": {
        "id": "11f11333"
      },
      "outputs": [],
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, embed_max_norm=None):\n",
        "        super().__init__()\n",
        "        ### YOUR CODE HERE ###\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim,\n",
        "                                      max_norm=embed_max_norm)\n",
        "        self.linear = nn.Linear(in_features=embed_dim,\n",
        "                                out_features=vocab_size)       \n",
        "        ### *** ###\n",
        "    \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Input:\n",
        "          x: torch.LongTensor - tensor with shape (batch,) containing the indices of the input words\n",
        "        Return value:\n",
        "          logits: torch.FloatTensor - tensor with shape (batch, vocab_size) containing unnormalized probabilities\n",
        "            for each word in the vocabulary\n",
        "        '''\n",
        "        ### YOUR CODE HERE ###\n",
        "        embedded = self.embedding(x)\n",
        "        logits = self.linear(embedded)\n",
        "        ### *** ###\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a69c0e6f",
      "metadata": {
        "id": "a69c0e6f"
      },
      "source": [
        "Implement the `fit()` function in the cell below. This function should return two lists, containing the loss and accuracy values over the training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "31879987",
      "metadata": {
        "id": "31879987"
      },
      "outputs": [],
      "source": [
        "def fit(model, train_loader, optimizer, **kwargs):\n",
        "    \n",
        "    num_epochs = kwargs.get('num_epochs', 100)\n",
        "    loss_fn = kwargs.get('loss_fn', nn.functional.cross_entropy)\n",
        "    device = kwargs.get('device', torch.device('cpu'))\n",
        "    \n",
        "    model.train()\n",
        "    train_loss_hist, train_acc_hist = [], []\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}')\n",
        "        pbar = tqdm(train_loader, total=len(train_loader))\n",
        "        train_loss, train_acc = 0, 0\n",
        "        for x, y in pbar:\n",
        "            ### YOUR CODE HERE ###\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = loss_fn(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            with torch.no_grad():\n",
        "                preds = logits.argmax(dim=1)\n",
        "            acc = (preds == y).float().sum() / ((y != -100).float().sum() + 1e-6)\n",
        "            ### *** ###\n",
        "            train_loss += loss.item()\n",
        "            train_acc += acc.item()\n",
        "            pbar.set_description(f'loss = {loss:.3f} | acc = {acc:.3f}')\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc /= len(train_loader)\n",
        "        print(f'train loss = {train_loss:.3f} | train acc = {train_acc:.3f}')\n",
        "        train_loss_hist.append(train_loss)\n",
        "        train_acc_hist.append(train_acc)\n",
        "        \n",
        "    return train_loss_hist, train_acc_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13da2d63",
      "metadata": {
        "id": "13da2d63"
      },
      "source": [
        "Now we instantiate the model, the optimizer and the dataloader and we're ready to train! As usual, we need to define a few hyperparameters. We set `EMBEDDING_DIM = 300` by default as this was the value used by the authors in the paper, but later you can play around with different values. Limiting the embeddings norm by setting `EMBEDDING_MAX_NORM = 1` is optional, but you can verify empirically that it will yield nicer embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "374c5cd7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "374c5cd7",
        "outputId": "bda5f451-b0a9-451b-8246-b951ff645885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE: cpu\n"
          ]
        }
      ],
      "source": [
        "EMBEDDING_DIM = 300\n",
        "EMBEDDING_MAX_NORM = 1\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 5\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device('cuda')\n",
        "# elif torch.backends.mps.is_available():\n",
        "#     DEVICE = torch.device('mps')\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "print('DEVICE:', DEVICE)\n",
        "\n",
        "model = SkipGramModel(vocab_size=len(vocab)-1, embed_dim=EMBEDDING_DIM, embed_max_norm=EMBEDDING_MAX_NORM).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce8d237",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ce8d237",
        "outputId": "8e1c8bdd-d44f-4956-a37b-f3f95b669863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss = 5.947 | acc = 0.047:  44%|████▍     | 36867/83184 [35:32<43:16, 17.84it/s]"
          ]
        }
      ],
      "source": [
        "train_loss, train_acc = fit(model, dataloader, optimizer, num_epochs=NUM_EPOCHS, device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37c15eaf",
      "metadata": {
        "id": "37c15eaf"
      },
      "outputs": [],
      "source": [
        "plt.title('Train loss')\n",
        "plt.plot(range(len(train_loss)), train_loss)\n",
        "plt.show()\n",
        "plt.title('Train accuracy')\n",
        "plt.plot(range(len(train_acc)), train_acc)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c356892",
      "metadata": {
        "id": "3c356892"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'word2vec.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02717b8e",
      "metadata": {
        "id": "02717b8e"
      },
      "source": [
        "Extract the embeddings from the model and normalize them to have unit L2-norm. Store the normalized embeddings in a `np.array` named `embeddings_norm`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7354b087",
      "metadata": {
        "id": "7354b087"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "### *** ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5caf1654",
      "metadata": {
        "id": "5caf1654"
      },
      "source": [
        "Now we'll **visualize the learned embeddings**! Since these are 300-dimensional, they are impossible to visualize directly. First, **we need to project the embeddings into a two- (or three-)dimensional space**. We do this by using the t-Distributed Stochastic Neighbor Embedding algorithm (**t-SNE**, https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), which usually provides very nice visualizations of high-dimensional data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b6b0254",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "0b6b0254",
        "outputId": "b9f43471-54ac-41eb-8802-26dfbb8ffe1a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-498670799103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membeddings_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# t-SNE transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# get embeddings\n",
        "embeddings_df = pd.DataFrame(embeddings)\n",
        "\n",
        "# t-SNE transform\n",
        "tsne = TSNE(n_components=2)\n",
        "embeddings_df_trans = tsne.fit_transform(embeddings_df)\n",
        "embeddings_df_trans = pd.DataFrame(embeddings_df_trans)\n",
        "\n",
        "# get token order\n",
        "embeddings_df_trans.index = [vocab_inv[i] for i in range(len(vocab_inv)-1)]\n",
        "\n",
        "# if token is a number\n",
        "is_numeric = embeddings_df_trans.index.str.isnumeric()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b003e5c9",
      "metadata": {
        "id": "b003e5c9"
      },
      "source": [
        "Run the cell below and open the generated file (word2vec_visualization.html) in your browser. Explore the embedding space. You should see semantically related words close to each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4a316bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "f4a316bc",
        "outputId": "383c2317-a158-4d3e-e509-e93ccbad2380"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7a089c88dbfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"green\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"black\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m fig.add_trace(\n\u001b[1;32m      5\u001b[0m     go.Scatter(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "color = np.where(is_numeric, \"green\", \"black\")\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=embeddings_df_trans[0],\n",
        "        y=embeddings_df_trans[1],\n",
        "        mode=\"text\",\n",
        "        text=embeddings_df_trans.index,\n",
        "        textposition=\"middle center\",\n",
        "        textfont=dict(color=color),\n",
        "    )\n",
        ")\n",
        "fig.write_html(\"./word2vec_visualization.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be582110",
      "metadata": {
        "id": "be582110"
      },
      "source": [
        "Implement the function `get_most_similar`, which receives a `word` and outputs a dictionary where the keys are the `topN` words whose embeddings are closer (as measured by the cosine similarity) to the embedding of the `word`. The values of the returned dictionary should be filled with the similarities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30bf2cbd",
      "metadata": {
        "id": "30bf2cbd"
      },
      "outputs": [],
      "source": [
        "def get_most_similar(word, vocab, vocab_inv, embeddings, topN=1):\n",
        "    ### YOUR CODE HERE ###\n",
        "    \n",
        "    ## *** ###\n",
        "    return topN_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48590839",
      "metadata": {
        "id": "48590839"
      },
      "outputs": [],
      "source": [
        "for word in ['mother', 'portugal', 'queen', 'sports']:\n",
        "    print(f\"Top-10 words most similar to '{word}': {get_most_similar(word, vocab, vocab_inv, embeddings_norm, topN=10)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f97bd2",
      "metadata": {
        "id": "e6f97bd2"
      },
      "source": [
        "## Negative sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c56c8d06",
      "metadata": {
        "id": "c56c8d06"
      },
      "source": [
        "Our vocabulary has only ~4k words, so training without negative sampling was doable. However, it is very common to have much larger vocabularies (10^5-10^7 words) where the original word2vec model is much more expensive to train.\n",
        "\n",
        "For this reason, you'll now implement the skip-gram Word2Vec model with negative sampling. You should start by implementing a new dataset class in the cell below, which extends the original `SkipGramDataset`. The `SkipGramDatasetNS` should have the following attributes:\n",
        "1. `inputs: List[int]` - same as before\n",
        "2. `contexts: torch.LongTensor` - tensor with shape `(num_examples, num_neg + 1)` that contains the index of the positive word (true context) and the indices of the negative words, which are sampled from the vocabulary according to the distribution provided in the list `word_probs`. You can use the function `np.random.choice` to do the sampling.\n",
        "3. `labels: torch.FloatTensor` - tensor with shape (num_neg + 1) that is 1 in the position corresponding to the positive word and 0 elsewhere.\n",
        "\n",
        "**Remark:** We could be generating negative examples *on-the-fly* (i.e., in `__getitem__`) instead of generating them only once in `__init__`. This way we would avoid repeating the same negative examples every training epoch, which could improve the results a bit. However, `np.random.choice` is quite slow and therefore on-the-fly generation would slow down training considerably."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a63c779",
      "metadata": {
        "id": "6a63c779"
      },
      "outputs": [],
      "source": [
        "class SkipGramDatasetNS(SkipGramDataset):\n",
        "    def __init__(self, train_iter, vocab, word_probs, window_width, num_neg):\n",
        "        super().__init__(train_iter, vocab, window_width)\n",
        "        ### YOUR CODE HERE ###\n",
        "        \n",
        "        ### *** ###\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        Return values:\n",
        "          input: int - the index of the input word \n",
        "          contexts: torch.LongTensor - tensor with shape (num_neg + 1,) containing the indices of the positive\n",
        "            and negative words\n",
        "          labels: torch.FloatTensor - tensor with binary labels with the same shape as contexts\n",
        "        '''\n",
        "        return self.inputs[index], self.contexts[index], self.labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dd44624",
      "metadata": {
        "id": "9dd44624"
      },
      "source": [
        "We can think of many distributions to sample our negatives from. The authors of wav2vec found empirically that a good option is to sample negative words $w$ from the distribution $q$ defined by: $$q(w) = \\frac{\\hat{p}(w)^{3/4}}{Z},$$ where $\\hat{p}$ is the distribution of words in the training corpus and $Z$ is a normalization constant (such that $\\sum_{\\forall w \\in V} q(w) = 1$). The fractional exponent (3/4) has a smoothing effect on the distribution, so that we avoid sampling the most frequent words too often.\n",
        "\n",
        "In the cell below, build the list `word_probs` where `word_probs[i]`$= q(w_i)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d6ff99d",
      "metadata": {
        "id": "6d6ff99d"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "### *** ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f466b44",
      "metadata": {
        "id": "4f466b44"
      },
      "source": [
        "Now we instantiate our dataset. We just need to decide the number of negative words to use. A small value (<10) is usually enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f74b49e",
      "metadata": {
        "id": "6f74b49e"
      },
      "outputs": [],
      "source": [
        "NUM_NEG = 5\n",
        "\n",
        "datasetNS = SkipGramDatasetNS(train_iter, vocab, word_probs, WINDOW_WIDTH, NUM_NEG)\n",
        "print(f'Dataset has {len(datasetNS)} examples.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782ba439",
      "metadata": {
        "id": "782ba439"
      },
      "source": [
        "The model architecture and the `fit` routine also need to be changed. How? That's what you need to think about out now :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e2765e",
      "metadata": {
        "id": "e4e2765e"
      },
      "outputs": [],
      "source": [
        "class SkipGramModelNS(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, embed_max_norm=None):\n",
        "        super().__init__()\n",
        "        ### YOUR CODE HERE ###\n",
        "        \n",
        "        ### *** ###\n",
        "    \n",
        "    def forward(self, x, c):\n",
        "        '''\n",
        "        Inputs:\n",
        "          x: torch.LongTensor - tensor with shape (batch,) containing the indices of the input words\n",
        "          c: torch.LongTensor - tensor with shape (batch, num_context) containing the indices of the context words\n",
        "        Return value:\n",
        "          sim: torch.FloatTensor - tensor with shape (batch, num_context) containing the cosine similarities between\n",
        "            the embeddings of x and the embeddings of each context word in c.\n",
        "        '''\n",
        "        ### YOUR CODE HERE ###\n",
        "        \n",
        "        ### *** ###\n",
        "        return sim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15839753",
      "metadata": {
        "id": "15839753"
      },
      "outputs": [],
      "source": [
        "def fitNS(model, train_loader, optimizer, **kwargs):\n",
        "    ### YOUR CODE HERE ###\n",
        "    \n",
        "    ### *** ###\n",
        "        \n",
        "    return train_loss_hist, train_acc_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aec651a5",
      "metadata": {
        "id": "aec651a5"
      },
      "source": [
        "Now, we're ready to go! Due to negative sampling, this model should train a bit faster than the previous one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4374a5a6",
      "metadata": {
        "id": "4374a5a6"
      },
      "outputs": [],
      "source": [
        "modelNS = SkipGramModelNS(vocab_size=len(vocab)-1, embed_dim=EMBEDDING_DIM, embed_max_norm=EMBEDDING_MAX_NORM).to(DEVICE)\n",
        "optimizerNS = optim.Adam(modelNS.parameters(), lr=LEARNING_RATE)\n",
        "dataloaderNS = DataLoader(datasetNS, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bcb4fba",
      "metadata": {
        "id": "0bcb4fba"
      },
      "outputs": [],
      "source": [
        "train_lossNS, train_accNS = fitNS(modelNS, dataloaderNS, optimizerNS, num_epochs=NUM_EPOCHS, device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60a026fe",
      "metadata": {
        "id": "60a026fe"
      },
      "outputs": [],
      "source": [
        "plt.title('Train loss (with neg. sampling)')\n",
        "plt.plot(range(len(train_lossNS)), train_lossNS)\n",
        "plt.show()\n",
        "plt.title('Train accuracy (with neg. sampling)')\n",
        "plt.plot(range(len(train_accNS)), train_accNS)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad96b13a",
      "metadata": {
        "id": "ad96b13a"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'word2vecNS.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8559a2ac",
      "metadata": {
        "id": "8559a2ac"
      },
      "source": [
        "Now we repeat the experiments we have done before for the model without negative sampling. You can copy the code for normalizing the embeddings and paste it in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c947480",
      "metadata": {
        "id": "1c947480"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "### *** ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1506f252",
      "metadata": {
        "id": "1506f252"
      },
      "outputs": [],
      "source": [
        "# get embeddings\n",
        "embeddings_df = pd.DataFrame(embeddings)\n",
        "\n",
        "# t-SNE transform\n",
        "tsne = TSNE(n_components=2)\n",
        "embeddings_df_trans = tsne.fit_transform(embeddings_df)\n",
        "embeddings_df_trans = pd.DataFrame(embeddings_df_trans)\n",
        "\n",
        "# get token order\n",
        "embeddings_df_trans.index = [vocab_inv[i] for i in range(len(vocab_inv)-1)]\n",
        "\n",
        "# if token is a number\n",
        "is_numeric = embeddings_df_trans.index.str.isnumeric()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f68930eb",
      "metadata": {
        "id": "f68930eb"
      },
      "outputs": [],
      "source": [
        "color = np.where(is_numeric, \"green\", \"black\")\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=embeddings_df_trans[0],\n",
        "        y=embeddings_df_trans[1],\n",
        "        mode=\"text\",\n",
        "        text=embeddings_df_trans.index,\n",
        "        textposition=\"middle center\",\n",
        "        textfont=dict(color=color),\n",
        "    )\n",
        ")\n",
        "fig.write_html(\"./word2vecNS_visualization.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f332a08",
      "metadata": {
        "id": "4f332a08"
      },
      "outputs": [],
      "source": [
        "for word in ['mother', 'portugal', 'queen', 'sports']:\n",
        "    print(f\"Top-10 words most similar to '{word}': {get_most_similar(word, vocab, vocab_inv, embeddings_norm, topN=10)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6adc197a",
      "metadata": {
        "id": "6adc197a"
      },
      "source": [
        "## Playground"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a55fc753",
      "metadata": {
        "id": "a55fc753"
      },
      "source": [
        "Now you can have some fun finding other interesting relations in the embeddings you have learned. There are also a few hyperparameters that we have defined rather arbitrarily that you can play with.\n",
        "With little effort, you can also implement the **continuous bag-of-words (CBOW)** version of Word2Vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98f085a",
      "metadata": {
        "id": "f98f085a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}