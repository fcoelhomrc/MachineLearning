# Machine Learning - 27/09/2022

# Regression

## Introduction and notation

The goal of this class is to discuss parametric models, where the usual steps involve:

- **Model representation**: the mathematical description of our model in terms of its parameters
- **Goal function**: a function of the model current outputs and the true results (data), with which we will be able to tune our model's parameters according to available data
- **Optimization**: apply optimization algorithm to find optimal values for the model's parameters such that the cost function is minimized

The dataset can be represented as a $N \times D$ matrix $X$, whose $N$ rows represent different observations, each consisting of $D$ measured attributes (features)

Our notation shall use **superscripts for features** (columns) and **subscripts for observations**.

Observations are represented as **column vectors**:
$$
x_i = [x_i^{(1)}, x_i^{(2)}, \dots, x_i^{(D)}]^T
$$
such that the matrix $X$ can be written as:
$$
X = [x_1, x_2, \dots, x_N]
$$

Outputs are equally given as **column vectors**
$$
y = [y_1, y_2, \dots, y_N]^T
$$

## Linear models

We start with the linear models, which prescribe a linear relationship between data and output:

$$
\hat{y} = w_0 + w_1 x^{(1)} + w_2 x^{(2)} + \cdots + w_D x^{(D)}
$$

Notice that we write $\hat{y} = f(x)$ to make it explicit that these are our predicted values.

Geometrically, this model representation is **a hyperplane in D-1 dimensional space**. If we only have a single feature, its a line. For two features, a plane. 

How to choose the best model $f(x; \theta)$? We try to minimize a given metric. For example, we may choose to look for a model which minimizes:

$$
\mathcal{L}(y, \hat{y}) = \underbrace{\sum_{n=1}^{N}\left(y_n - \hat{y}_n\right)^2}_{\text{Data fitting term}}
$$

Now that we have our goal formalized, we can write the desired model as:

$$
f^*(x; \theta) = \argmin_{\theta \in \Theta} \mathcal{L (y, \hat{y}(x; \theta))}
$$

...where $\theta$ represent our model parameters. In this case, 
$$
\theta = [ w_0, w_1, \dots, w_D ]
$$

Changing up a bit the notation to match the professor's:
$$
w^* = \text{optimal vector of weights}\\
    = \argmin_{\mathcal{W}} \sum_{n=1}^{N}\left(y_n - ( w_0 + w_1 x_n^{(1)} + w_2 x_n^{(2)} + \cdots + w_D x_n^{(D)})\right)^2 
$$

This is just a quadratic function! It is very easy to optimize: compute the zeroes of the first derivative with respect to each weight.

### Algebra refresher

If $a, b$ are D-dimensional column vectors, we have:

$$
f(a, b) = \sum_{i=1}^{D} a_ib_i = a^T b = b^T a
$$

and also 

$$
\frac{\partial f}{\partial a} = b, \qquad \frac{\partial f}{\partial b} = a
$$

Another important result is the following:

$$
f(w) = f([w_1, w_2]^T) = w^T M w \\
\frac{df}{dw} = 2Mw
$$

Now lets go back to our optimization problem!

$$
\begin{align}
e = [e_1, \dots, e_N]^T &= [\dots, y_i - (w_0 + w_1 x_i^{(1)} + \cdots + w_D x_i^{(D)}), \dots]^T \\
&= y - (Xw), \qquad \text{forcing bias = 0 for now}
\end{align}
$$

Our cost function is simply:

$$
f(w) = (y - Xw)^T(y - Xw) = y^Ty - y^TXw - w^TX^Ty + w^TX^TXw
$$

And we have:

$$
\frac{df}{dw} = 0 + (X^T y) + (X^T y) + 2(X^T X)w
$$

...using the formulas in the Algebra Refresher.

Finding the zeroes:

$$
2X^T y = 2(X^T X)w \Leftrightarrow w = (X^T X)^{-1} X^T y
$$

Thus, the optimal parameters are simply:

$$
w = (X^T X)^{-1} X^T y
$$

and our prediction is $\hat{y} = w^Tx$ (the inner product) for a new input $x$.

If we now consider non-zero bias, 

