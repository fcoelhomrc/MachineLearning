# Machine Learning - 13/09/2022

## Introduction to Machine Learning

Common Computer Vision tasks: classification, detection, segmentation

- Classification: label image given a set of possible classes
- Detection: label and find different objects inside a given image
- Segmentation: label and find objects with a detailed outline

Supervised learning examples: classification, denoising, OCR...

- Classfication: output space is discrete

- Regression: output space is a continuous value
  -  Denoising: remove noise from given output

- Structured prediction

## Taxonomy of problem types
Our focus is on: 
- Supervised learning
  - Classification
    - Binary
    - Multiclass
      - Nominal
      - Ordinal
  - Regression 

## Developing a model
Typical tasks involve mathematically modelling equations describing some kind of $y = f(x)$

Types of models:
- Fixed models: we are able to conceptually design without requirement of data
- **Parametric models:** model + data driven
- **Non-parametric models:** data driven

Typical Physics textbook problem solution is an example of Fixed Model $\rightarrow$ Simple, solved problems

### The 3 generations of AI design
- 1st wave: 
    - Rule-based in order to emulate expert decision-making, leading to an interpretable model
    - Cannot be dynimically improved with data, cannot generalize, cannot adapt to uncertainty in the problem
- 2nd wave:
  - Moved to Parametric Modelling by using data to estimate/optimize the parameters required by a Fixed Model-style formulation
  - Data completes model design
  - The goal is not to *replicate* the if-else structures of expert decision making, but to *approximate* them
  - The expert's knowledge is important to *design the features* that will allow to capture the data information into the model

### Example of complete ML system

- Sensor: captures data
- Preprocessing: filters and prepare data
- Feature Extraction: expert knowledge exctracts information from data
- Decision: use features to solve the problem

Our focus will be in the **decision** step

## Example: problem involving multiple features
**Task:** detect cancer given lesion image 

This is a binary classification task.

Assume a 2D feature space:

$x_1, x_2 =$ lightness, length

Dataset: 

$$
Obs_n = (x_1^{n}, x_2^{n})
$$
$$
y_{n} = label_n, \qquad label_n \in \{\text{cancer}, \text{not cancer}\}
$$

By computing a decision boundary (in this case, a line), we can divide the feature space into two regions and associate to each a label

$$
f(x_1, x_2) = k
$$

where $k$ is constant

E.g calculate average $x_1, x_2$ for cancer and not-cancer classes, use as decision boundary the normal crossing the midpoint between average cases.

General case is $N$-dimensional feature space, boundary becomes a $N-1$ hyperplane (for binary classification)

This will only solve the problem if it is linearly separable.

More complex models would be non-linear.

Remember always that what matter is the quality of the predictions for the complete population, not to just past observations

i.e we want a model that work for ANY input $\mathbf{x}$

A model that performs really well in the observed data may not be the best when considering new data

**Overfitting**: the model learns the required information from the dataset, but also picks up specificities of the training dataset.

This happens if your model is too complex and is dangerous because the resulting model won't generalize well.

Learning $\ne$ Memorizing training data

**Nearest Neighbours:**
Given input, find datapoint in available training set that is closest to it. The input label is the same as the nearest neighbour label.


- Simply saves the training data in memory. The training is really fast.

- The prediction is really slow because it will compare new data to each stored datapoint. 

- It will also be sensitive to noise, because outliers may be chosen as nearest neighbour.

## Data driven design
- Data should be cheap to collect
- Problem is too complex for fixed models
- If the data is not representative of the population, the model will have trouble generalizing

## Design of a classifier

Example (cancer detection, part 2)

Attempt to model the probability distribution:

$$
p(y = \text{cancer}|\mathbf{x = x_0}) = p_0
$$

where $p_0$ is your decision threshold.

The model may then be restricted to making a decision only when it is sure ($p_0$ sufficiently large), and letting human experts tackle the more complicated cases.

What if an outlier evaluates to $p = 0.9999$? If all cancer cases are clustered around a region, this outlier may be an anomaly that should be studied separately. Our model, as it is, would classify this instance as cancer imediatelly.

What if we also consider:

$$
p(\mathbf{x = x_0}|y = \text{cancer})
$$

This would allow us to also have **anomaly detection**, since the previous troublesome case would evaluate:

$$
p(\mathbf{x = x_0}|y = \text{cancer}) \ll 1
$$

## Common steps in model design
- **Model representation:** for example, a linear model, etc.
- **Goal function (Loss/Cost or Fitness):** allows us to evaluate how well a given model performed
- **Optimization:** using the defined measure, find the best performing model