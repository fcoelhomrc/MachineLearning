# Advanced Topics in Machine Learning - 01/10/2022

Topics covered in this module:

- Recurrent Neural Networks (RNNs)
- Introduction to NLP
  - Assignment #2
- Attention Mechanism and the Transformer
- Self-supervised Learning and Large Language Models (LLMs)

# Recurrent Neural Networks (RNNs)

## Introduction

So far, the networks were memoryless. In RNNs there is a state $\mathbf{h}$ which enables the network to remember previous states e.g time series processing

This temporal dynamics is made possible by a feedback mechanism:
$$
\mathbf{h^{(t)}} = f(\mathbf{x^{(t)}}, \mathbf{h^{(t-1)}})
$$

This function $f$ is the usual affine transformation followed by a *squashing function* (activation function), e.g $\tanh$ or $\sigma$
$$
\mathbf{h^{(t)}} = \tanh (\mathbf{U}\mathbf{x^{(t)}} + \mathbf{W}\mathbf{h^{(t-1)}} + \mathbf{b})
$$

Notice that the way this is written assumes causality (only previous states affect current state), and it is formulated to depent explicitly only on the previous state (but since the previous state depends on the previos etc., there is a complete temporal relationship implicitly defined).

**See Handwritten Notes, figure #1**

[![](https://mermaid.ink/img/pako:eNqVkU0LgkAQhv_KMqcClfK4QlDkLTTUm-th0SnFT7YVkvK_t7VhQUE0p-F9n2cOuxdI2wyBwlHwLie7wGENURNG6yCKSUJMc3VlkM8WcwZXsokDz0u-M-fZUjPPejO5Oncf7ptqv7u2ZnTtTqqOw8jfx5ZlKVsDJzlUqK-QQ1FVZtvxtJADXRiqE22Jr8T5UOx_HX9PfitgQI2i5kWmnvNyP8BA5lgjA6rWjIuSAWtGxfFetuHQpECl6NGAvsu4xG3B1S_UQA-8OuF4AyzkfXM)](https://mermaid.live/edit#pako:eNqVkU0LgkAQhv_KMqcClfK4QlDkLTTUm-th0SnFT7YVkvK_t7VhQUE0p-F9n2cOuxdI2wyBwlHwLie7wGENURNG6yCKSUJMc3VlkM8WcwZXsokDz0u-M-fZUjPPejO5Oncf7ptqv7u2ZnTtTqqOw8jfx5ZlKVsDJzlUqK-QQ1FVZtvxtJADXRiqE22Jr8T5UOx_HX9PfitgQI2i5kWmnvNyP8BA5lgjA6rWjIuSAWtGxfFetuHQpECl6NGAvsu4xG3B1S_UQA-8OuF4AyzkfXM)

## Training

For training, backpropagation can still be used. Let us see an example:

Considerar input sequence $\mathbf{x} = [\mathbf{x}^{(1)}, \dots,\mathbf{x}^{(T)}]$ and labels $\mathbf{y} = [\mathbf{y}^{(1)}, \dots,\mathbf{y}^{(T)}]$, together with the loss function 
$$
\mathcal{L} = \sum_{t=1}^{T}\mathcal{L}^{t} = \sum_{t=1}^{T} l(\mathbf{y}^{(t)}, f(\mathbf{x}^{(t)}, \mathbf{h}^{(t-1)}; \mathbf{\theta})
$$

And the optimization problem is 

$$
\argmin_{\mathbf{\theta}} \sum_{t=1}^{T} l(\mathbf{y}^{(t)}, f(\mathbf{x}^{(t)}, \mathbf{h}^{(t-1)}; \mathbf{\theta})
$$

with the algorithm using $\mathbf{\theta} \leftarrow \mathbf{\theta} - \eta \nabla_{\mathbf{\theta}} \mathcal{L}$

## The problem of long-term memory

It is hard for the RNN to learn long-term dependencies, i.e. it tends to forget information that is too far away temporally

If we consider a network without non-linearity and without bias, i.e $\mathbf{h}^{(t)} = \mathbf{W}\mathbf{h}^{(t-1)}$, which implies

$$
\mathbf{h}^{(t)} = \mathbf{W}^t\mathbf{h}^{(0)}
$$

Imagine that $\mathbf{h}, \mathbf{W}$ are scalars. For $t$ big,there are two possibilities:

1. If $|\mathbf{W}| > 1, \mathbf{h}^{(t)} \rightarrow \infty$
2. If $|\mathbf{W}| < 1, \mathbf{h}^{(t)} \rightarrow 0$

These assymptotic behaviors are exponential.

Of course, if we are dealing with the matricial case, **it is $\mathbf{W}$ eigenvalues that dictade this behavior**.

If we consider the nonlinear $\tanh$, it will not explode since it is a bounded function (in this case, saturates at $\pm 1$). We can't use a $\text{ReLU}$ since it would diverge!

We thus solve the first case, but the vanishing case remains! And it is really hard to solve, leading the network to forget past information!

A similar problem arises in training! The gradients also tend to vanish.

## Solutions

There are two approaches:
- For exploding gradients, apply **gradient clipping**
- For vanishing gradients, **LSTMs**

# Long Short Term Memory cells (LSTMs)

They introduce the concept of **gate vectors**, which combine to give a more complex internal state $(\mathbf{c}^{(t)}, \mathbf{h}^{(t)})$.

**See definitions at slides**

Keep in mind $\tanh{z} \in [-1, 1]$ while $\sigma{(z)} \in [0, 1]$.

There are 4 gate vectors, 3 of them are sigmoids $\in [0, 1]$:
$$
\begin{align}
\mathbf{i}^{(t)} &\rightarrow \text{input gate}\\
\mathbf{f}^{(t)} &\rightarrow \text{forget gate} \\
\mathbf{o}^{(t)} &\rightarrow \text{output gate}
\end{align}
$$
and finally, one is $\tanh \in [-1, 1]$:
$$
\mathbf{g}^{(t)} \rightarrow \text{normal RNN cell}
$$

The **input gate** is essentially binary, 0 or 1, **and tells us wheter we have the cell working as a normal RNN or we close it.**

The **forget gate** is also binary, and **decides if we keep what we have learned so far to the next state, or if we discard everything.**

The **output gate** is again binary and decides **if we use the state during calculation of the output, or not.**

**One thing is to update state, another is to use current state to compute output**

This gives a finer control of the temporal information, at the cost of training 4 times as many parameters.

# Architectures

- Many-to-one
  - Feed a single input $x^{(1)}$
  - Run the RNN until $t=T$ to generate output sequence $y^{(1)}, y^{(2)}, \dots, y^{(T)}$
  - Optionally, feed output back at input at each time step $x^{(t+1)}=y^{(t)}$

- Many-to-many
  - Input/output with same length $T$
  - Feed input sequence $x^{(1)}, x^{(2)}, \dots, x^{(T)}$
  - Get output sequence $y^{(1)}, y^{(2)}, \dots, y^{(T)}$, one term after each time step

- Many-to-many
  - Input/output with different lengths $T_{out} \ne T_{in}$
  - Feed input sequence $x^{(1)}, x^{(2)}, \dots, x^{(T)}$
  - Ignore output for all time steps up to $T_{in} - 1$
  - At each time step $t \ge T_{in}$, get output $y^{(t - (T_in - 1))}$

- Encoder/Decoder
  - LSTMs network for processing input (**encoder**), another for generating output (**decoder**) 
  - Output of encoder is called **context**
  - Each step generated by decoder depends on context, previous decoder output and decoder state
  - These concepts are still the base of state-of-the-art architectures
  
## Bi-LSTMs

In order to process time series **bidirectionally** from $t = 0 \rightarrow T$ and also $t = T \rightarrow 0$ (both ways), we use this model which basically consists of **two LSTMs in parallel, processing the series in each direction and concatenating the results afterwards**

## Attention mechanism

This allows the decoder to look at different parts of the input at different instants, as before it could only look at the final context output.

This is important in NLP, for example: we need information in a different place inside a sentence to decide the correct gender to use for a word.

Essentially, instead of using the same context vector for each decoding step, we use different contexts to each step.

For example, at encoder we can weight each $h^{(t)}$ with a parameter $\alpha^{(t, t')}$, which are chosen through a softmax activation. **Note that this attention matrix is not learned! It is generated on the fly with a set of functions depending on encoder/decoder output sequences**.



# Comments

**Transfer learning plays an important role in state-of-the-art networks.**

For example, we usually **pre-train linguistic models in a unsupervised manner**, so that it learns low-level semantics and grammar.

For example, grab text online, randomly mask words and ask the model to predict them. This will allow the model to learn which words go together, etc.

Only then we train the model at a specific task, e.g summarizing text. If we attempt to train it from scratch, the required data would be brutal since the model actually would have to learn these more basic tasks first.