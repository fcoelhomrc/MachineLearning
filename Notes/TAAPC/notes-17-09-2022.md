# Advanced Topics on ML - 17/09/2022

# Neural Networks + Deep Learning

## Logistic Regression

- Linear regression:
  - Can be used in ML for prediction tasks
  - $y = \alpha + \beta x$, Least Squares for finding $\beta$
- Multiple Lin reg.
  - $y = \alpha + \sum_i\beta_i x_i$

For discrete output var., it doesn't make sense to use LinReg

Try to approximate $p(y|x)$ instead.

**Logistic function**
$$
p(y|x) = \frac{e^{\alpha + \beta x}}{1 + e^{\alpha + \beta x}}
$$
S-shaped, bounded $p \in [0, 1]$

How do we fit the function to our data?

### Maximum Likelihood Estimation (MLE)

**(see handwritten notes #1)**

$$
\hat{\theta} = \argmax_{\theta \in \Theta} \mathcal{L}(\theta|y)
$$

where $\theta$ are the parameters we want to estimate

- We can choose any underlying distribution
- Our $\theta$ can be their respective moments
  - E.g For a Gaussian, $\theta = (\mu, \sigma)$

We usually use negative log-likelihood for convenience since most prob. dist. are from the exponential family

**NOTE:** Refresh MLE, exponential family by reading Casella's Statistical Inference

### Training LogReg
**(see handwritten notes #2)**

The results are
- MLE for $\alpha, \beta$
- Estimates of $p(y|x)$: this allows us to perform classification once we define a decision boundary

How to define decision boundary? $p(y|x) = 0.5$, assuming the negative/positive classes are equivalent.

If they are not, then we can weight them by picking a boundary other than $0.5$

### Multiple LogReg

For many independent varaibles:

$$
\ln{\frac{p}{1-p}} = \alpha + \sum_i^N \beta_i x_i
$$

Mathematical details are in the slides

### Regularization

Penalize large coefficients to avoid overfitting

i.e we also try to minimize one of these:

- L1 Reg: 
  - $|\alpha| + \sum_i^N |\beta_i|$
- L2 Reg:
  -  $\alpha^2 + \sum_i^N \beta_i^2$
- Elastic-net Reg:
  -  $c_1 L1 + c_2 L2$

Notice that the larger $(\alpha, \beta_i)$ are, the larger these cost functions are

### Notes on decision boundary

For Logistic Reg., the decision boundary is always linear

## Neural Networks

Mimic the brain:
Nodes $\rightarrow$ Neurons, Links $\rightarrow$ Synapses

### Perceptron

Weights $w$ and bias $b$

$$
\sum w_i x_i  =  y_i = 
\begin{cases}
+1, y_i > 0 \\
-1, \, \text{else}
\end{cases} = \text{sgn}(\textstyle\sum w_i x_i)
$$

This is just a linear model, restricted to linear problems

$$
y = \text{sgn}(w_T x)
$$

Error function:

- 0-1 Loss:

Only update weights when we fail:
$$
w_{t+1} = w_{t} + \Delta w_{t}, \\
\Delta w_{t}= \eta(y_n - y(x_n))x_n
$$

$\eta$ is called **learning rate**: how much we change weights at each step $\rightarrow$ can be tuned to speed training

**Note:** $\eta$ needs not to be constant!

A good strategy is to start with larger $\eta$ and then decrease it as we approach the local minimum to avoid overshooting and oscillating around it.

Notice that this is a **hyperparameter**

### Sign/Step function
Bad:
- Not differentiable
- Small change in input leads to large change in output

### Better activation functions
- Sigmoid 
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

- Tanh

These are good because they are smooth, differentiable.

These have limitations when it comes to Gradient Descent since they have vanishing gradient at the boundary of their domains $\rightarrow$ problematic when training with GD

- ReLU
$$
\max{(0, x
)}
$$

Tries to fix vanishing gradient problem.

- etc. (See slides)

We can look at all these activation function options as **hyperparameters** of the network.

Think about the output layer in our network:

- For binary classification, a sigmoid makes sense
- For regression, we need a linear function!
- Regression with bounded output (e.g pixel intensity): use a sigmoid and then map to desired range

### Multi-layer Perceptron (MLP)

**Network architecture:** layers + connections

The accepted idea is that **deeper is better than wider**: increase number of layers, not their size

**Note:** these models usually **work better with normalized inputs**! This avoids problems with numeric representation in computers

Usually we want the network to **become wider as we get deeper**. The starting layers encode **low level features**; the intermediate layers combine those to form **high-level features**, which are more specific and detailed. Thus, it is useful to have more of them.

This model is basically an **universal function approximator**.

All the operations between layers are just matrix multiplications. GPUs are special hardware designed for fast/large matrix multiplication

### Network structure

- Feed-forward network
  - No internal state: there is no memory of past predictions used for the current prediction
  - Directed *acyclic* graph: simply computes outputs from inputs

- Recurrent network
  - Directed *cyclic* graph: can memorize information since it is a dynamical system with internal states

## Training Neural Networks

General approach is to move along parameters space in the direction of negative gradient
$$
w_{n} = w_{n-1} - \gamma \nabla f(w_n), \\
\text{stop if max it or } \nabla f = 0
$$
For multi-variate functions the gradient is generalized as the Jacobian matrix

















